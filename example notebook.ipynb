{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada2e9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if you do not have multiset library installed\n",
    "# pip install multiset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977a69dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pylab as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import bernoulli\n",
    "from multiset import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "from oracles import LogReg\n",
    "\n",
    "from nodes import Nodes\n",
    "\n",
    "from utils import generate_synthetic, default_dataset_parameters, read_data\n",
    "from utils import save_run_func, load_run_func\n",
    "from utils import nonconvex_reg, l2_reg\n",
    "from utils import run_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed820c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'w7a'\n",
    "problem = 'logreg'\n",
    "dataset_path = './Datasets/{}.txt'.format(data_name)\n",
    "save_path = f''\n",
    "\n",
    "\n",
    "# regularization parameter\n",
    "lmb = 1e-1\n",
    "sigma = 0.1\n",
    "\n",
    "# choose default parameters \n",
    "N = default_dataset_parameters[data_name]['N']# size of the whole data set\n",
    "n = default_dataset_parameters[data_name]['n']# number of nodes\n",
    "m = default_dataset_parameters[data_name]['m']# size of local data set\n",
    "d = default_dataset_parameters[data_name]['d']# number of weights of the model\n",
    "\n",
    "# or set them manually\n",
    "N = default_dataset_parameters[data_name]['N']# size of the whole data set\n",
    "n = 10\n",
    "m = N//n\n",
    "d = default_dataset_parameters[data_name]['d']# number of weights of the model\n",
    "\n",
    "parameters = {'N':N, 'n':n, 'm':m, 'd':d, 'lmb':lmb}\n",
    "\n",
    "# data reading\n",
    "A, b = read_data(dataset_path=dataset_path,\n",
    "                 N=N, n=n, m=m, d=d, lmb=lmb,\n",
    "                labels=['+1', '-1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae60b3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the problem\n",
    "logreg = LogReg(X=A, y=b, n=n, m=m, batch_size=None, reg=nonconvex_reg, reg_coef=lmb)\n",
    "\n",
    "# define the starting point\n",
    "np.random.seed(42)\n",
    "x0 = np.ones(d)\n",
    "\n",
    "np.linalg.norm(logreg.full_gradient(x0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c5e820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define computing speeds\n",
    "computing_speeds = np.array(range(1,n+1))\n",
    "computing_speeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392dbe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the delay type from {normal, fixed, poisson, uniform}\n",
    "delay_type = 'normal'\n",
    "\n",
    "# set the server seed to make the results reproducable\n",
    "server_seed = 1\n",
    "\n",
    "# set the batch size: integer not larger than m or None to use full local dataset\n",
    "batch_size = None\n",
    "\n",
    "# define the computiong nodes \n",
    "nodes = Nodes(num_nodes=n, computing_speeds=computing_speeds, oracle=logreg, batch_size=batch_size, b=1,\n",
    "              delay_type=delay_type, server_seed=server_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e815a1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimization method\n",
    "# and job assigning procedure assign_type for it from {pure, random, shuffle}\n",
    "# here x0 is starting point\n",
    "# gamma is a stepsize\n",
    "# num_iter is the number of iterations\n",
    "\n",
    "async_opt_pure = lambda gamma: run_optimizer(x0, nodes, gamma, num_iter=500, assign_type='pure')\n",
    "async_opt_random = lambda gamma: run_optimizer(x0, nodes, gamma, num_iter=500, assign_type='random')\n",
    "async_opt_shuffle = lambda gamma: run_optimizer(x0, nodes, gamma, num_iter=500, assign_type='shuffle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bd3650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an array of stepsizes to check\n",
    "gamma_grid = np.logspace(-1, -10, num=10, base=2)\n",
    "gamma_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893c3d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_pure = []\n",
    "delays_pure = []\n",
    "for gamma in tqdm(gamma_grid):\n",
    "    \n",
    "    # run the method with chosen stepsize\n",
    "    er, de = async_opt_pure(gamma)\n",
    "    errors_pure.append(er)\n",
    "    delays_pure.append(de)\n",
    "    \n",
    "    # update the results\n",
    "    run = {'pure_error':errors_pure, 'pure_delay':delays_pure}\n",
    "    \n",
    "    # save current results\n",
    "    save_run_func(f'pure_n.{n}_m.{m}_d.{d}_lmb.{lmb}_{delay_type}'\\\n",
    "                  +f'_batch_{batch_size}_seed_{server_seed}_stochastic_grad',run)\n",
    "    \n",
    "# for each stepsize gamma run consists of two lists:\n",
    "# the first one collects the gradient norms for each run with specific gamma \n",
    "# the second one collects the delays for each run with specific gamma"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
